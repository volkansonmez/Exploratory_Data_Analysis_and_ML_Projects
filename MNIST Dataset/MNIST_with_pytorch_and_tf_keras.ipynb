{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook by [Volkan Sonmez](http://www.pythonicfool.com/)  \n",
    "### MNIST Dataset is viewed with matplotlib.image library \n",
    "### CNN models were built with Pytorch, Tensorflow (TF_Keras) and their performances were compared\n",
    "\n",
    "####  https://github.com/volkansonmez/Exploratory_Data_Analysis_and_ML_Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [License](#License)\n",
    "\n",
    "3. [EDA and Building ML Models](#EDA_and_Building_ML_Models)\n",
    "    \n",
    "4. [Pytorch_Model](#Pytorch_Model)\n",
    "   \n",
    "5. [Tensorflow.Keras_Model](#Tensorflow.Keras_Model)\n",
    "\n",
    "7. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "    \n",
    "MNIST dataset can be downloaded from: https://www.kaggle.com/oddrationale/mnist-in-csv  \n",
    "\n",
    "MNIST is a large dataset of handwritten digits (0 to 9) that is commonly used for training various image processing systems containing 60,000 training images and 10,000 testing images. Each image has 28x28 pixels. \n",
    "\n",
    "In this notebook, the MNIST dataset was trained on 2 different packages and their performances were tested. Both models have identical arhitecture. The order for passing the image through model was picked as: 2 x (Conv Layer, Regularization, Max Pool Layer, Activation Function) and then a fully connected layer. The logits of FCL were then passed to the softmax function. The cross-entropy loss was used since this is a multi-class logistic regression problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Please see the [repository README file](https://github.com/volkansonmez/Exploratory_Data_Analysis_and_ML_Projects) \n",
    "for the licenses and usage terms for the instructional material and code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA_and_Building_ML_Model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "How to do it: \n",
    "\n",
    "The dataset is in .csv format so it is very easy to read it with pandas library. View some of the instances with the matplotlib.pyplot.imshow module. \n",
    "\n",
    "Use torch.utils.data.DataLoader module to make iterable batches of training and test sets for the pytorch model. \n",
    "\n",
    "Using Pytorch: Write a model by using torch.nn module. Here is a rule of thumb architecture for a very simple model: 2 x (Conv Layer, Regularization, Max Pool Layer, Activation Function) + 1 FCL + Softmax. The optimizer can be chosen Adams and the cross-entropy loss function should be used since this is a multi-class logistic regression problem. With a grid search for the best learning rate or learning rate decay, the accuracy of this model can be improved. For the sake of simplicity, hyper-parameters were not defined but the default values were used both for the optimizer and loss functions in the notebook. \n",
    "\n",
    "\n",
    "Using Tensorflow-Keras: Like building pytorch's easy-to-built sequential model, tensorflow has keras module. The same architecture above can be written just with 10 lines of code. \n",
    "\n",
    "Compare the results of these two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explatory Data Analysis of MNIST and the Machine Learning Modeling of MNIST Dataset with PyTorch and Tf-Keras\n",
    "\n",
    "# Import the necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST to the same working folder. Load the data.\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785) (28000, 784)\n",
      "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
      "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
      "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
      "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
      "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
      "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
      "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
      "\n",
      "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
      "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
      "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
      "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
      "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
      "\n",
      "       pixel781  pixel782  pixel783  \n",
      "count   42000.0   42000.0   42000.0  \n",
      "mean        0.0       0.0       0.0  \n",
      "std         0.0       0.0       0.0  \n",
      "min         0.0       0.0       0.0  \n",
      "25%         0.0       0.0       0.0  \n",
      "50%         0.0       0.0       0.0  \n",
      "75%         0.0       0.0       0.0  \n",
      "max         0.0       0.0       0.0  \n",
      "\n",
      "[8 rows x 785 columns]\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    4\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View the properties of the data\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "print(train.describe())\n",
    "print(train['label'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f93fcb3fc90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the target values in the data\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "X_train = train.drop(\"label\", axis = 1) # all the data except the labels\n",
    "print(sorted(Y_train.unique()))\n",
    "sns.countplot(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000,)\n",
      "4132\n"
     ]
    }
   ],
   "source": [
    "# Turn the dataframe into a numpy array\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "print(Y_train.shape) # check the shape\n",
    "print(len(X_train[Y_train == 0])) # view the number of instances represent 0 on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM20lEQVR4nO3db6hc9Z3H8c9nY6Joo0RzlYuVTbf4YGVx03KJYkpRgvEPalKka++DJAti+sBAC3lgqEL9C7JuWxZZirdr7F2pKYU2Jg+kW4kBqUj1RqKJe9mNf7JNasidIP6pD0yTfvvgHrvXeOfMdc6ZOZP7fb9gmJnznXPO1zGfnJnzO5OfI0IA5r+/aboBAP1B2IEkCDuQBGEHkiDsQBJn9HNnS5cujWXLlvVzl0AqBw8e1LFjxzxbrVLYbV8v6d8kLZD0HxHxcNnrly1bpomJiSq7BFBiZGSkba3rj/G2F0j6d0k3SLpM0qjty7rdHoDeqvKdfYWkNyLirYg4LunnktbU0xaAulUJ+8WSDs14frhY9im2N9qesD3RarUq7A5AFVXCPttJgM9cexsRYxExEhEjQ0NDFXYHoIoqYT8s6ZIZz78o6Z1q7QDolSphf1nSpba/ZHuRpG9J2llPWwDq1vXQW0ScsL1J0n9peuhta0S8XltnAGpVaZw9Ip6R9ExNvQDoIS6XBZIg7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJvk7ZDPTTqlWr2taee+650nXHx8dL6+vXr++qpyZxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnx2nrmmuuKa2/8MILbWu2S9ftVD8dVQq77YOSPpR0UtKJiBipoykA9avjyH5NRByrYTsAeojv7EASVcMekn5je4/tjbO9wPZG2xO2J1qtVsXdAehW1bCvjIivSrpB0p22v37qCyJiLCJGImJkaGio4u4AdKtS2CPineJ+StJ2SSvqaApA/boOu+1zbC/+5LGk1ZL219UYgHpVORt/kaTtxXjkGZKeiohf19IVIOnBBx8srb/44oul9RMnTrSt3XbbbaXr3nrrraX101HXYY+ItyT9Y429AOghht6AJAg7kARhB5Ig7EAShB1Igp+4ojFPP/10af2hhx4qrR8/fry0fvnll7etjY2Nla579tlnl9ZPRxzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnRU4cOHWpbu++++0rX/fjjj0vrF1xwQWn9gQceaFtbvHhx6brzEUd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcXZU8tJLL5XW77jjjra1ffv2Vdr3o48+Wlq/+eabK21/vuHIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6OUk8++WRpff369aX1YkrvWZ133nml61577bWl9euuu660jk/reGS3vdX2lO39M5adb/tZ2weK+yW9bRNAVXP5GP9TSdefsmyLpF0RcamkXcVzAAOsY9gj4nlJ756yeI2k8eLxuKS1NfcFoGbdnqC7KCKOSFJxf2G7F9reaHvC9kSr1epydwCq6vnZ+IgYi4iRiBgZGhrq9e4AtNFt2I/aHpak4n6qvpYA9EK3Yd8paUPxeIOkHfW0A6BXOo6z294m6WpJS20flvR9SQ9L+oXt2yX9XtI3e9kkeufo0aOl9UceeaRn+167tvy87hNPPNGzfWfUMewRMdqmtKrmXgD0EJfLAkkQdiAJwg4kQdiBJAg7kAQ/cZ3n3nvvvdL66tWrS+v79+8vrXdy7rnntq3dcsstlbaNz4cjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTj7PPfRRx+V1qtOm9zJoUOH2tYWL17c033j0ziyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLPPA8eOHWtbu+mmm0rXjYhK+77yyitL64sWLaq0fdSHIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+zywadOmtrVXX321dF3bpfWrrrqqtL5r167S+plnnllaR/90PLLb3mp7yvb+Gcvutf0H23uL2429bRNAVXP5GP9TSdfPsvxHEbG8uD1Tb1sA6tYx7BHxvKR3+9ALgB6qcoJuk+3Xio/5S9q9yPZG2xO2J1qtVoXdAaii27D/WNKXJS2XdETSD9q9MCLGImIkIkaGhoa63B2AqroKe0QcjYiTEfFnST+RtKLetgDUrauw2x6e8fQbkqrN6wug5zqOs9veJulqSUttH5b0fUlX214uKSQdlPTtHvaYXtnv1SXpzTff7HrbnX5vvmXLltI64+inj45hj4jRWRY/3oNeAPQQl8sCSRB2IAnCDiRB2IEkCDuQBD9xHQBTU1Ol9dHR2QZE/t+ePXva1s4666zSdR977LHSeqd/ihqnD47sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wDYPv27aX13bt3d73tK664orS+bt26rreN0wtHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2Pti2bVtp/a677qq0/ZUrV7atPfXUU5W2jfmDIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4ew3ef//90vo999xTWv/ggw8q7X/z5s1ta8PDw5W2jfmj45Hd9iW2d9uetP267e8Uy8+3/aztA8X9kt63C6Bbc/kYf0LS5oj4e0lXSrrT9mWStkjaFRGXStpVPAcwoDqGPSKORMQrxeMPJU1KuljSGknjxcvGJa3tVZMAqvtcJ+hsL5P0FUm/k3RRRByRpv9CkHRhm3U22p6wPdFqtap1C6Brcw677S9I+qWk70bEnM8oRcRYRIxExMjQ0FA3PQKowZzCbnuhpoP+s4j4VbH4qO3hoj4sqXwqUgCN6jj0ZtuSHpc0GRE/nFHaKWmDpIeL+x096fA0sGNH+X/622+/3dP9Vx26Qw5zGWdfKWmdpH229xbLvqfpkP/C9u2Sfi/pm71pEUAdOoY9In4ryW3Kq+ptB0CvcLkskARhB5Ig7EAShB1IgrADSfAT1xosXLiwtL5gwYLS+smTJ0vrZ5xR/r/pwIEDpXVA4sgOpEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzl6D0dHR0vr9999fWu80zn733XeX1jds2FBaBySO7EAahB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsfTA5Odl0CwBHdiALwg4kQdiBJAg7kARhB5Ig7EAShB1IomPYbV9ie7ftSduv2/5Osfxe23+wvbe43dj7dgF0ay4X1ZyQtDkiXrG9WNIe288WtR9FxL/2rj0AdZnL/OxHJB0pHn9oe1LSxb1uDEC9Ptd3dtvLJH1F0u+KRZtsv2Z7q+0lbdbZaHvC9kSr1arULIDuzTnstr8g6ZeSvhsRH0j6saQvS1qu6SP/D2ZbLyLGImIkIkaGhoZqaBlAN+YUdtsLNR30n0XEryQpIo5GxMmI+LOkn0ha0bs2AVQ1l7PxlvS4pMmI+OGM5cMzXvYNSfvrbw9AXeZyNn6lpHWS9tneWyz7nqRR28slhaSDkr7dkw4B1GIuZ+N/K8mzlJ6pvx0AvcIVdEAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQcEf3bmd2S9H8zFi2VdKxvDXw+g9rboPYl0Vu36uztbyNi1n//ra9h/8zO7YmIGGmsgRKD2tug9iXRW7f61Rsf44EkCDuQRNNhH2t4/2UGtbdB7Uuit271pbdGv7MD6J+mj+wA+oSwA0k0Enbb19v+H9tv2N7SRA/t2D5oe18xDfVEw71stT1le/+MZefbftb2geJ+1jn2GuptIKbxLplmvNH3runpz/v+nd32Akn/K+laSYclvSxpNCL+u6+NtGH7oKSRiGj8AgzbX5f0R0n/GRH/UCz7F0nvRsTDxV+USyLirgHp7V5Jf2x6Gu9itqLhmdOMS1or6Z/V4HtX0tc/qQ/vWxNH9hWS3oiItyLiuKSfS1rTQB8DLyKel/TuKYvXSBovHo9r+g9L37XpbSBExJGIeKV4/KGkT6YZb/S9K+mrL5oI+8WSDs14fliDNd97SPqN7T22NzbdzCwuiogj0vQfHkkXNtzPqTpO491Pp0wzPjDvXTfTn1fVRNhnm0pqkMb/VkbEVyXdIOnO4uMq5mZO03j3yyzTjA+Ebqc/r6qJsB+WdMmM51+U9E4DfcwqIt4p7qckbdfgTUV99JMZdIv7qYb7+atBmsZ7tmnGNQDvXZPTnzcR9pclXWr7S7YXSfqWpJ0N9PEZts8pTpzI9jmSVmvwpqLeKWlD8XiDpB0N9vIpgzKNd7tpxtXwe9f49OcR0febpBs1fUb+TUl3N9FDm77+TtKrxe31pnuTtE3TH+v+pOlPRLdLukDSLkkHivvzB6i3JyXtk/SapoM13FBvX9P0V8PXJO0tbjc2/d6V9NWX943LZYEkuIIOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5L4C60I1i2aMpm/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View a random instance with imshow module.\n",
    "\n",
    "plt.imshow(X_train[0].reshape(28,28), cmap = plt.cm.binary)\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View a sample instance from all 10 different labels\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True)\n",
    "for i in range(10):\n",
    "    img = X_train[Y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img,  cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the different instances under the same category (target value)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True,)\n",
    "for i in range(10):\n",
    "    img = X_train[Y_train == 1][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Explatory Data Analysis and Beginning of the NN model with CNNs ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch_Model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is: 2.359494686126709\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.368865042924881\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.49539193511009216\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.008758824318647385\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.055711984634399414\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.03974331170320511\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.0011269596870988607\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.00043945611105300486\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.0005080607370473444\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.005118937697261572\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.000921172380913049\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.013299629092216492\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.06519930064678192\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.025570472702383995\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "loss is: 0.00026745122158899903\n",
      "shape of input torch.Size([10, 1, 28, 28])\n",
      "accuracy is: 98.31999969482422\n"
     ]
    }
   ],
   "source": [
    "# Using Torch and CNN, here is the NN architecture to develop a ML model that can predict digits with 98% accuracy: \n",
    "\n",
    "# Upload the data from torchvision datasets instead of using the dataset downloaded earlier.\n",
    "# Build this model: input ---> conv1, dropout1, maxpool1, relu1, conv2, dropout2, maxpool2, fcl, softmax ---> output\n",
    "\n",
    "# Here is the size and shape of each layer and the explanation of the forward propagation:\n",
    "# 1 x 28 x 28 ---> (1,10) 5x5 conv1 ----> 10 x 24 x 24 -----> dropout----> maxpool1(2,2) ----> 10 x 12 x 12 ----> \n",
    "# relu ----> (10,20) 5x5 conv2 ---->  20 x 8 x 8  -----> dropout -----> maxpool2(2,2) ----> 20 x 4 x 4 ----> \n",
    "# fcl (320, 10) ----> softmax <----> LOSS & View the results\n",
    "\n",
    "# Note: Use relu activation function right after maxpools for regularization to achieve best results.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Download and normalize the training dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', download=True, train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                          transforms.ToTensor(), \n",
    "                                                          transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                          ])), batch_size=10, shuffle=True)\n",
    "\n",
    "# Download and normalize the testing dataset\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', download=True, train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), \n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) \n",
    "                                                          ])), batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size = 5) # in , out, kernel size\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size = 5) # in , out, kerner size\n",
    "        self.fcl1 = torch.nn.Linear(320, 10) # in, out \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # input 1 x 28 x 28 ---> (1,10) 5x5 conv1 ----> output 10 x 24 x 24 \n",
    "        x = torch.nn.functional.dropout(x) # 10 x 24 x 24 \n",
    "        x = torch.nn.functional.max_pool2d(x, 2) # maxpool1(2,2) ----> 10 x 12 x 12 \n",
    "        x = torch.relu(x) # 10 x 12 x 12 \n",
    "        x = self.conv2(x) # input 10 x 12 x 12 ----> (10,20) 5x5 conv2 ----> output 20 x 8 x 8 \n",
    "        x = torch.nn.functional.dropout(x) # 20 x 8 x 8 \n",
    "        x = torch.nn.functional.max_pool2d(x, 2) # maxpool2(2,2) ----> 20 x 4 x 4\n",
    "        x = torch.relu(x) # 20 x 4 x 4\n",
    "        x = x.view(-1, 320) # input 20 x 4 x 4 ----> output (1 x 320)\n",
    "        x = self.fcl1(x) # input (1 x 320)----> output (10)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1) # output softmax of 10 scores \n",
    "        return x\n",
    "        \n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model_parameters = {}\n",
    "\n",
    "def train(epoch, save = False):\n",
    "    net.train() # this is the built-in \"train\" function of the sequential NN model.\n",
    "    for ep in range(epoch):\n",
    "        for idx , (image, label) in enumerate(train_loader): \n",
    "            # forward pass\n",
    "            fwd = net.forward(image)\n",
    "            # loss\n",
    "            loss = criterion(fwd, label)\n",
    "            # print loss\n",
    "            if idx % 6000 == 0: # each batch is 64 instances\n",
    "                print('loss is: {}'.format(loss))\n",
    "                # print('shape of input', image.shape)\n",
    "            # zero the grads\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "        if save:\n",
    "        # save the trained model parameters\n",
    "            model_parameters = {'model': net.state_dict() , 'optim' : optimizer.state_dict()}\n",
    "            path = 'model_params{}.pth'.format(ep) # save the model parameters under the current working directory\n",
    "            torch.save(model_parameters , path)\n",
    "            print('model parameters is saved: {}'.format(path))\n",
    "    #test the data\n",
    "    test()\n",
    "        \n",
    "        \n",
    "def test():\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx , (image, label) in enumerate(test_loader):\n",
    "            # forward pass\n",
    "            logit = net(image)\n",
    "            # calculate loss\n",
    "            test_loss += criterion(logit, label).sum()\n",
    "            # compare logit and actual results\n",
    "            pred_idx = logit.max(1, keepdim = True)[1] # it is the np.argmax on axis = 1\n",
    "            correct += pred_idx.eq(label.view_as(pred_idx)).sum()\n",
    "    accuracy = 100 * correct / float(len(test_loader.dataset))\n",
    "    print('accuracy is: {}'.format(accuracy))\n",
    "    \n",
    "\n",
    "        \n",
    "train(15, save = False) # intentionally overfitted to see the loss value fluctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow.Keras_Model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Using TF Keras for the same problem with a pre-fixed 2 fcl architecture for warm up:\n",
    "# Uploading data from keras datasets instead of using the torchvision dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist # 28 x 28 images of hand-written digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f93ad733fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap = plt.cm.binary) # see if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "[  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape) # taking a peek at the shape of the array \n",
    "print(x_train[0][10]) # the numbers in the array are between 0-254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the array\n",
    "x_train = tf.keras.utils.normalize(x_train, axis = 1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
      " 0.33153488 0.11664776 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2599\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.1029\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0712\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0516\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.0386\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0316\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0250\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0208\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0162s \n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0171\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0143\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0119\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.0113\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.01030s - loss: 0.0 - ETA: 0s - l\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f93ab8cf990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUT ---> FCL, RELU, FCL, RELU, SOFTMAX ---> OUTPUT\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer ='adam', loss = 'sparse_categorical_crossentropy', metris = ['accuracy'])\n",
    "model.fit(x_train, y_train, epochs = 15) # intentionally overfitted to see the loss value fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.1360\n",
      "0.13598931129725503\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test, verbose = 1)) # view the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = np.array(model.predict(x_test))\n",
    "test_labels = np.array(y_test)\n",
    "test_results = np.argmax(test_results, axis = 1)\n",
    "assert (len(test_results == test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "print(np.mean(test_results[:] == test_labels[:])) # This model is missing the maxpool layers. Let's add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Now, it is time to build a good convolutional network with Keras. Like the one built with pytorch above\n",
    "# Here is the original code at Keras for MNIST:\n",
    "# https://keras.io/examples/vision/mnist_convnet/  \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      "54000/54000 [==============================] - 45s 831us/sample - loss: 0.3607 - acc: 0.8900 - val_loss: 0.0864 - val_acc: 0.9775\n",
      "Epoch 2/15\n",
      "54000/54000 [==============================] - 46s 860us/sample - loss: 0.1149 - acc: 0.9650 - val_loss: 0.0582 - val_acc: 0.9853\n",
      "Epoch 3/15\n",
      "54000/54000 [==============================] - 49s 909us/sample - loss: 0.0868 - acc: 0.9732 - val_loss: 0.0496 - val_acc: 0.9875\n",
      "Epoch 4/15\n",
      "54000/54000 [==============================] - 41s 757us/sample - loss: 0.0725 - acc: 0.9776 - val_loss: 0.0426 - val_acc: 0.9892\n",
      "Epoch 5/15\n",
      "54000/54000 [==============================] - 46s 851us/sample - loss: 0.0649 - acc: 0.9801 - val_loss: 0.0389 - val_acc: 0.9902\n",
      "Epoch 6/15\n",
      "54000/54000 [==============================] - 44s 811us/sample - loss: 0.0555 - acc: 0.9831 - val_loss: 0.0365 - val_acc: 0.9898\n",
      "Epoch 7/15\n",
      "54000/54000 [==============================] - 36s 664us/sample - loss: 0.0507 - acc: 0.9842 - val_loss: 0.0382 - val_acc: 0.9888\n",
      "Epoch 8/15\n",
      "54000/54000 [==============================] - 38s 713us/sample - loss: 0.0483 - acc: 0.9846 - val_loss: 0.0327 - val_acc: 0.9910\n",
      "Epoch 9/15\n",
      "54000/54000 [==============================] - 41s 752us/sample - loss: 0.0435 - acc: 0.9863 - val_loss: 0.0335 - val_acc: 0.9907\n",
      "Epoch 10/15\n",
      "54000/54000 [==============================] - 42s 777us/sample - loss: 0.0428 - acc: 0.9865 - val_loss: 0.0327 - val_acc: 0.9910\n",
      "Epoch 11/15\n",
      "54000/54000 [==============================] - 40s 741us/sample - loss: 0.0398 - acc: 0.9873 - val_loss: 0.0315 - val_acc: 0.9922\n",
      "Epoch 12/15\n",
      "54000/54000 [==============================] - 36s 658us/sample - loss: 0.0374 - acc: 0.9879 - val_loss: 0.0306 - val_acc: 0.9917\n",
      "Epoch 13/15\n",
      "54000/54000 [==============================] - 36s 674us/sample - loss: 0.0352 - acc: 0.9887 - val_loss: 0.0276 - val_acc: 0.9923\n",
      "Epoch 14/15\n",
      "54000/54000 [==============================] - 39s 728us/sample - loss: 0.0345 - acc: 0.9892 - val_loss: 0.0300 - val_acc: 0.9927\n",
      "Epoch 15/15\n",
      "54000/54000 [==============================] - 42s 775us/sample - loss: 0.0344 - acc: 0.9894 - val_loss: 0.0281 - val_acc: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f937e0dff10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.023373040785791818\n",
      "Test accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# save the model if needed\n",
    "# model.save('mnist_2nn_keras')\n",
    "# create a new model with the saved model \n",
    "# new_model = tf.keras.models.load_model('mnist_2nn_keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Both models built with pytorch and tensorflow-keras, have around 98-99 percent accuracy for the MNIST dataset. Tuning the hyper-parameters would yield even better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
