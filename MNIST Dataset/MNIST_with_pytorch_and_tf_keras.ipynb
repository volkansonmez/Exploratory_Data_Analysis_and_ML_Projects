{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook by [Volkan Sonmez](http://www.pythonicfool.com/)  \n",
    "### Digits recognition with Pytorch and Tensorflow (TF_Keras) on MNIST Dataset\n",
    "##### [Pythonicfool GitHub Repository](https://github.com/volkansonmez/Exploratory_Data_Analysis_and_ML_Projects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [EDA and Building ML Models](#EDA_and_Building_ML_Models)\n",
    "    \n",
    "3. [Pytorch_Model](#Pytorch_Model)\n",
    "   \n",
    "4. [Tensorflow.Keras_Model](#Tensorflow.Keras_Model)\n",
    "\n",
    "5. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "MNIST is a large dataset of handwritten digits (0 to 9) that is commonly used for training various image processing systems containing 60,000 training images and 10,000 testing images. Each image has 28x28 pixels. Recognizing digits is a multi-class logistic regression problem. \n",
    "    \n",
    "> MNIST dataset can be downloaded from: https://www.kaggle.com/oddrationale/mnist-in-csv  \n",
    "\n",
    "In this notebook, the MNIST dataset was trained on PyTorch and Keras (Tensorflow) packages, and their performances were tested. Both models have identical architecture. The order of layers was picked with this configuration: \n",
    "> (2 x (Conv Layer, Regularization, Max Pool Layer, Activation Function) + a fully connected layer.) \n",
    "\n",
    "The logits of FCL were then passed to the softmax function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA_and_Building_ML_Model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "This notebook uses several Python packages that come standard with the Anaconda Python distribution. \n",
    "The primary libraries you need to run this notebook are:\n",
    "\n",
    "* **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "* **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn**: The essential Machine Learning package in Python.\n",
    "* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **Seaborn**: Advanced statistical plotting library.\n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "    conda install numpy pandas matplotlib tensorflow pytorch seaborn\n",
    "\n",
    "How to do it: \n",
    "\n",
    "The dataset is in .csv format so it is very easy to read it with pandas library. View some of the instances with the matplotlib.pyplot.imshow module. \n",
    "\n",
    "Use torch.utils.data.DataLoader module to create iterable batches of training and test sets for the PyTorch model. \n",
    "\n",
    ">Using Pytorch: \n",
    "\n",
    "Create a model by using the torch.nn module. Here is a rule of thumb architecture for a very simple model: 2 x (Conv Layer, Regularization, Max Pool Layer, Activation Function) + 1 FCL + Softmax. The cross-entropy loss function should be used since this is a multi-class logistic regression problem. For the sake of simplicity, default hyper-parameters were used both for the optimizer and loss functions.\n",
    "\n",
    "\n",
    ">Using Tensorflow-Keras: \n",
    "\n",
    "Like building pytorch's easy-to-build sequential model, TensorFlow has Keras module. The same architecture above can be written just with 10 lines of code. \n",
    "\n",
    "Compare the results of these two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis of MNIST and the Machine Learning Modeling of MNIST Dataset with PyTorch and Tf-Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset to the same working folder. Load the data into pandas dataframe.\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785) (28000, 784)\n",
      "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
      "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
      "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
      "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
      "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
      "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
      "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
      "\n",
      "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
      "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
      "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
      "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
      "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
      "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
      "\n",
      "       pixel781  pixel782  pixel783  \n",
      "count   42000.0   42000.0   42000.0  \n",
      "mean        0.0       0.0       0.0  \n",
      "std         0.0       0.0       0.0  \n",
      "min         0.0       0.0       0.0  \n",
      "25%         0.0       0.0       0.0  \n",
      "50%         0.0       0.0       0.0  \n",
      "75%         0.0       0.0       0.0  \n",
      "max         0.0       0.0       0.0  \n",
      "\n",
      "[8 rows x 785 columns]\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    4\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View the properties of the dataframe\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "print(train.describe())\n",
    "print(train['label'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fef195c6f90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the target values in the data\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "X_train = train.drop(\"label\", axis = 1) # all the data except the labels\n",
    "print(sorted(Y_train.unique()))\n",
    "sns.countplot(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000,)\n",
      "4132\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataframe into an n-dimensional numpy array\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "print(Y_train.shape) # check the shape\n",
    "print(len(X_train[Y_train == 0])) # view the number of instances represent 0 on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View a random instance with imshow module.\n",
    "\n",
    "plt.imshow(X_train[0].reshape(28,28), cmap = plt.cm.binary)\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View a sample instance from all 10 different labels\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True)\n",
    "for i in range(10):\n",
    "    img = X_train[Y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img,  cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the different instances under the same category (target value)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True,)\n",
    "for i in range(10):\n",
    "    img = X_train[Y_train == 1][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Explatory Data Analysis and Beginning of the NN model with CNNs ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch_Model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Torch and CNN, here is the NN architecture to develop a ML model that can predict digits with 98% accuracy: \n",
    "\n",
    "Upload the data from torchvision datasets instead of using the dataset downloaded earlier.\n",
    "Build this model: input ---> conv1, dropout1, maxpool1, relu1, conv2, dropout2, maxpool2, fcl, softmax ---> output\n",
    "\n",
    "Here is the size and shape of each layer and the explanation of the forward propagation:\n",
    "\n",
    "1 x 28 x 28 ---> (1,10) 5x5 conv1 ----> 10 x 24 x 24 -----> dropout----> maxpool(2,2) ----> 10 x 12 x 12 ----> \n",
    "relu ----> (10,20) 5x5 conv2 ---->  \n",
    "\n",
    "20 x 8 x 8  -----> dropout -----> maxpool2(2,2) ----> 20 x 4 x 4 ----> fcl (320, 10) ----> softmax <----> LOSS \n",
    "\n",
    "Note: Use relu activation function right after maxpools for regularization to achieve best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a713ac079c4b77b9f0c9db5fe267c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a558d583dc6a42658920032ef603da5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3de7dfd61e4828afe941f517592b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1bbd0487204c3b817d4085a29dbaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "loss is: 2.2543179988861084\n",
      "loss is: 0.1970173716545105\n",
      "loss is: 0.22992321848869324\n",
      "loss is: 0.056348931044340134\n",
      "loss is: 2.9585873562609777e-05\n",
      "loss is: 0.7805323600769043\n",
      "loss is: 0.0032805223017930984\n",
      "loss is: 0.0011950669577345252\n",
      "loss is: 0.038494713604450226\n",
      "loss is: 0.014764678664505482\n",
      "loss is: 0.0004260221030563116\n",
      "loss is: 0.0009565182263031602\n",
      "loss is: 8.678420272190124e-05\n",
      "loss is: 0.00027247617254033685\n",
      "loss is: 0.006663521286100149\n",
      "accuracy is: 98.0199966430664\n"
     ]
    }
   ],
   "source": [
    "# Create the model with Pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Download and normalize the training dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', download=True, train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                          transforms.ToTensor(), \n",
    "                                                          transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                          ])), batch_size=10, shuffle=True)\n",
    "\n",
    "# Download and normalize the testing dataset\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', download=True, train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), \n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) \n",
    "                                                          ])), batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size = 5) # in , out, kernel size\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size = 5) # in , out, kerner size\n",
    "        self.fcl1 = torch.nn.Linear(320, 10) # in, out \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # input 1 x 28 x 28 ---> (1,10) 5x5 conv1 ----> output 10 x 24 x 24 \n",
    "        x = torch.nn.functional.dropout(x) # 10 x 24 x 24 \n",
    "        x = torch.nn.functional.max_pool2d(x, 2) # maxpool1(2,2) ----> 10 x 12 x 12 \n",
    "        x = torch.relu(x) # 10 x 12 x 12 \n",
    "        x = self.conv2(x) # input 10 x 12 x 12 ----> (10,20) 5x5 conv2 ----> output 20 x 8 x 8 \n",
    "        x = torch.nn.functional.dropout(x) # 20 x 8 x 8 \n",
    "        x = torch.nn.functional.max_pool2d(x, 2) # maxpool2(2,2) ----> 20 x 4 x 4\n",
    "        x = torch.relu(x) # 20 x 4 x 4\n",
    "        x = x.view(-1, 320) # input 20 x 4 x 4 ----> output (1 x 320)\n",
    "        x = self.fcl1(x) # input (1 x 320)----> output (10)\n",
    "        x = torch.nn.functional.log_softmax(x, dim = 1) # output softmax of 10 scores \n",
    "        return x\n",
    "        \n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model_parameters = {}\n",
    "\n",
    "def train(epoch, save = False):\n",
    "    net.train() # this is the built-in \"train\" function of the sequential NN model.\n",
    "    for ep in range(epoch):\n",
    "        for idx , (image, label) in enumerate(train_loader): \n",
    "            # forward pass\n",
    "            fwd = net.forward(image)\n",
    "            # loss\n",
    "            loss = criterion(fwd, label)\n",
    "            # print loss\n",
    "            if idx % 6000 == 0: # each batch is 64 instances\n",
    "                print('loss is: {}'.format(loss))\n",
    "                # print('shape of input', image.shape)\n",
    "            # zero the grads\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "        if save:\n",
    "        # save the trained model parameters\n",
    "            model_parameters = {'model': net.state_dict() , 'optim' : optimizer.state_dict()}\n",
    "            path = 'model_params{}.pth'.format(ep) # save the model parameters under the current working directory\n",
    "            torch.save(model_parameters , path)\n",
    "            print('model parameters is saved: {}'.format(path))\n",
    "    #test the data\n",
    "    test()\n",
    "        \n",
    "        \n",
    "def test():\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx , (image, label) in enumerate(test_loader):\n",
    "            # forward pass\n",
    "            logit = net(image)\n",
    "            # calculate loss\n",
    "            test_loss += criterion(logit, label).sum()\n",
    "            # compare logit and actual results\n",
    "            pred_idx = logit.max(1, keepdim = True)[1] # it is the np.argmax on axis = 1\n",
    "            correct += pred_idx.eq(label.view_as(pred_idx)).sum()\n",
    "    accuracy = 100 * correct / float(len(test_loader.dataset))\n",
    "    print('accuracy is: {}'.format(accuracy))\n",
    "    \n",
    "\n",
    "        \n",
    "train(15, save = False) # this is an intentional overfit to observe the fluctuations in the loss values below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow.Keras_Model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF Keras for the same problem with a pre-fixed 2 fcl architecture for warm up:\n",
    "# Uploading data from keras datasets instead of using the torchvision dataset\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist # 28 x 28 images of hand-written digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feec6a9b9d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOUElEQVR4nO3dX4xUdZrG8ecFwT8MKiyt2zJEZtGYIRqBlLAJG0Qni38SBS5mAzGIxogXIDMJxEW5gAsvjO7MZBQzplEDbEYmhJEIiRkHCcYQE0OhTAuLLGpapkeEIkTH0QsU373ow6bFrl81VafqlP1+P0mnquup0+dNhYdTXae6fubuAjD0DSt6AACtQdmBICg7EARlB4Kg7EAQF7RyZ+PGjfOJEye2cpdAKD09PTp58qQNlDVUdjO7XdJvJQ2X9Ly7P5G6/8SJE1UulxvZJYCEUqlUNav7abyZDZf0rKQ7JE2WtNDMJtf78wA0VyO/s0+X9IG7f+TupyX9QdLcfMYCkLdGyj5e0l/7fd+b3fYdZrbEzMpmVq5UKg3sDkAjGin7QC8CfO+9t+7e5e4ldy91dHQ0sDsAjWik7L2SJvT7/seSPmlsHADN0kjZ90q61sx+YmYjJS2QtD2fsQDkre5Tb+7+jZktk/Sa+k69vejuB3ObDECuGjrP7u6vSno1p1kANBFvlwWCoOxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiCIhlZxRfs7c+ZMMv/888+buv9169ZVzb766qvktocPH07mzz77bDJfuXJl1Wzz5s3JbS+66KJkvmrVqmS+Zs2aZF6EhspuZj2SvpB0RtI37l7KYygA+cvjyH6Lu5/M4ecAaCJ+ZweCaLTsLunPZrbPzJYMdAczW2JmZTMrVyqVBncHoF6Nln2mu0+TdIekpWY269w7uHuXu5fcvdTR0dHg7gDUq6Gyu/sn2eUJSdskTc9jKAD5q7vsZjbKzEafvS5pjqQDeQ0GIF+NvBp/paRtZnb257zk7n/KZaoh5ujRo8n89OnTyfytt95K5nv27KmaffbZZ8ltt27dmsyLNGHChGT+8MMPJ/Nt27ZVzUaPHp3c9sYbb0zmN998czJvR3WX3d0/kpR+RAC0DU69AUFQdiAIyg4EQdmBICg7EAR/4pqDd999N5nfeuutybzZf2baroYPH57MH3/88WQ+atSoZH7PPfdUza666qrktmPGjEnm1113XTJvRxzZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIzrPn4Oqrr07m48aNS+btfJ59xowZybzW+ejdu3dXzUaOHJncdtGiRckc54cjOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EwXn2HIwdOzaZP/XUU8l8x44dyXzq1KnJfPny5ck8ZcqUKcn89ddfT+a1/qb8wIHqSwk8/fTTyW2RL47sQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAE59lbYN68ecm81ufK11peuLu7u2r2/PPPJ7dduXJlMq91Hr2W66+/vmrW1dXV0M/G+al5ZDezF83shJkd6HfbWDPbaWZHssv0JxgAKNxgnsZvkHT7ObetkrTL3a+VtCv7HkAbq1l2d39T0qlzbp4raWN2faOk9PNUAIWr9wW6K939mCRll1dUu6OZLTGzspmVK5VKnbsD0Kimvxrv7l3uXnL3UkdHR7N3B6CKest+3Mw6JSm7PJHfSACaod6yb5e0OLu+WNIr+YwDoFlqnmc3s82SZksaZ2a9ktZIekLSFjN7QNJRST9v5pBD3aWXXtrQ9pdddlnd29Y6D79gwYJkPmwY78v6oahZdndfWCX6Wc6zAGgi/lsGgqDsQBCUHQiCsgNBUHYgCP7EdQhYu3Zt1Wzfvn3Jbd94441kXuujpOfMmZPM0T44sgNBUHYgCMoOBEHZgSAoOxAEZQeCoOxAEJxnHwJSH/e8fv365LbTpk1L5g8++GAyv+WWW5J5qVSqmi1dujS5rZklc5wfjuxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EATn2Ye4SZMmJfMNGzYk8/vvvz+Zb9q0qe78yy+/TG577733JvPOzs5kju/iyA4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQXCePbj58+cn82uuuSaZr1ixIpmnPnf+0UcfTW778ccfJ/PVq1cn8/HjxyfzaGoe2c3sRTM7YWYH+t221sz+Zmb7s687mzsmgEYN5mn8Bkm3D3D7b9x9Svb1ar5jAchbzbK7+5uSTrVgFgBN1MgLdMvMrDt7mj+m2p3MbImZlc2sXKlUGtgdgEbUW/bfSZokaYqkY5J+Ve2O7t7l7iV3L3V0dNS5OwCNqqvs7n7c3c+4+7eS1kuanu9YAPJWV9nNrP/fFs6XdKDafQG0h5rn2c1ss6TZksaZWa+kNZJmm9kUSS6pR9JDTZwRBbrhhhuS+ZYtW5L5jh07qmb33XdfctvnnnsumR85ciSZ79y5M5lHU7Ps7r5wgJtfaMIsAJqIt8sCQVB2IAjKDgRB2YEgKDsQhLl7y3ZWKpW8XC63bH9obxdeeGEy//rrr5P5iBEjkvlrr71WNZs9e3Zy2x+qUqmkcrk84FrXHNmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IAg+ShpJ3d3dyXzr1q3JfO/evVWzWufRa5k8eXIynzVrVkM/f6jhyA4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQXCefYg7fPhwMn/mmWeS+csvv5zMP/300/OeabAuuCD9z7OzszOZDxvGsaw/Hg0gCMoOBEHZgSAoOxAEZQeCoOxAEJQdCILz7D8Atc5lv/TSS1WzdevWJbft6empZ6Rc3HTTTcl89erVyfzuu+/Oc5whr+aR3cwmmNluMztkZgfN7BfZ7WPNbKeZHckuxzR/XAD1GszT+G8krXD3n0r6V0lLzWyypFWSdrn7tZJ2Zd8DaFM1y+7ux9z9nez6F5IOSRovaa6kjdndNkqa16whATTuvF6gM7OJkqZKelvSle5+TOr7D0HSFVW2WWJmZTMrVyqVxqYFULdBl93MfiTpj5J+6e5/H+x27t7l7iV3L3V0dNQzI4AcDKrsZjZCfUX/vbuf/TOo42bWmeWdkk40Z0QAeah56s3MTNILkg65+6/7RdslLZb0RHb5SlMmHAKOHz+ezA8ePJjMly1blszff//9854pLzNmzEjmjzzySNVs7ty5yW35E9V8DeY8+0xJiyS9Z2b7s9seU1/Jt5jZA5KOSvp5c0YEkIeaZXf3PZIGXNxd0s/yHQdAs/A8CQiCsgNBUHYgCMoOBEHZgSD4E9dBOnXqVNXsoYceSm67f//+ZP7hhx/WNVMeZs6cmcxXrFiRzG+77bZkfvHFF5/3TGgOjuxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EESY8+xvv/12Mn/yySeT+d69e6tmvb29dc2Ul0suuaRqtnz58uS2tT6uedSoUXXNhPbDkR0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgghznn3btm0N5Y2YPHlyMr/rrruS+fDhw5P5ypUrq2aXX355clvEwZEdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Iwd0/fwWyCpE2S/lnSt5K63P23ZrZW0oOSKtldH3P3V1M/q1QqeblcbnhoAAMrlUoql8sDrro8mDfVfCNphbu/Y2ajJe0zs51Z9ht3/6+8BgXQPINZn/2YpGPZ9S/M7JCk8c0eDEC+zut3djObKGmqpLOf8bTMzLrN7EUzG1NlmyVmVjazcqVSGeguAFpg0GU3sx9J+qOkX7r73yX9TtIkSVPUd+T/1UDbuXuXu5fcvdTR0ZHDyADqMaiym9kI9RX99+7+siS5+3F3P+Pu30paL2l688YE0KiaZTczk/SCpEPu/ut+t3f2u9t8SQfyHw9AXgbzavxMSYskvWdmZ9cefkzSQjObIskl9UhKr1sMoFCDeTV+j6SBztslz6kDaC+8gw4IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxBEzY+SznVnZhVJH/e7aZykky0b4Py062ztOpfEbPXKc7ar3X3Az39radm/t3OzsruXChsgoV1na9e5JGarV6tm42k8EARlB4IouuxdBe8/pV1na9e5JGarV0tmK/R3dgCtU/SRHUCLUHYgiELKbma3m9lhM/vAzFYVMUM1ZtZjZu+Z2X4zK3R96WwNvRNmdqDfbWPNbKeZHckuB1xjr6DZ1prZ37LHbr+Z3VnQbBPMbLeZHTKzg2b2i+z2Qh+7xFwtedxa/ju7mQ2X9L+S/l1Sr6S9kha6+/+0dJAqzKxHUsndC38DhpnNkvQPSZvc/frsticlnXL3J7L/KMe4+3+2yWxrJf2j6GW8s9WKOvsvMy5pnqT7VOBjl5jrP9SCx62II/t0SR+4+0fuflrSHyTNLWCOtufub0o6dc7NcyVtzK5vVN8/lparMltbcPdj7v5Odv0LSWeXGS/0sUvM1RJFlH28pL/2+75X7bXeu0v6s5ntM7MlRQ8zgCvd/ZjU949H0hUFz3Oumst4t9I5y4y3zWNXz/LnjSqi7AMtJdVO5/9muvs0SXdIWpo9XcXgDGoZ71YZYJnxtlDv8ueNKqLsvZIm9Pv+x5I+KWCOAbn7J9nlCUnb1H5LUR8/u4Judnmi4Hn+Xzst4z3QMuNqg8euyOXPiyj7XknXmtlPzGykpAWSthcwx/eY2ajshROZ2ShJc9R+S1Fvl7Q4u75Y0isFzvId7bKMd7VlxlXwY1f48ufu3vIvSXeq7xX5DyWtLmKGKnP9i6S/ZF8Hi55N0mb1Pa37Wn3PiB6Q9E+Sdkk6kl2ObaPZ/lvSe5K61VeszoJm+zf1/WrYLWl/9nVn0Y9dYq6WPG68XRYIgnfQAUFQdiAIyg4EQdmBICg7EARlB4Kg7EAQ/weypTV95ccHFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap = plt.cm.binary) # see if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "[  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape) # taking a peek at the shape of the array \n",
    "print(x_train[0][10]) # the numbers in the array are between 0-254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the array\n",
    "x_train = tf.keras.utils.normalize(x_train, axis = 1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
      " 0.33153488 0.11664776 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/volkansonmez/miniconda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.2670\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.1090s - l\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.07240s \n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0537\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0415\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0315\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0260\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0218\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0185\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.01500s - lo\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.0143\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0122\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0110\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0121\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feec6ae35d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUT ---> FCL, RELU, FCL, RELU, SOFTMAX ---> OUTPUT\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer ='adam', loss = 'sparse_categorical_crossentropy', metris = ['accuracy'])\n",
    "model.fit(x_train, y_train, epochs = 15) # intentionally overfitted to see the loss value fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.1392\n",
      "0.13918904686260297\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test, verbose = 1)) # view the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = np.array(model.predict(x_test))\n",
    "test_labels = np.array(y_test)\n",
    "test_results = np.argmax(test_results, axis = 1)\n",
    "assert (len(test_results == test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9738\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "print(np.mean(test_results[:] == test_labels[:])) # This model is missing the maxpool layers. Let's add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Now, it is time to build a good convolutional network with Keras. Like the one built with pytorch above\n",
    "# Here is the original code at Keras for MNIST:\n",
    "# https://keras.io/examples/vision/mnist_convnet/  \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      "54000/54000 [==============================] - 33s 609us/sample - loss: 0.3662 - acc: 0.8891 - val_loss: 0.0799 - val_acc: 0.9785\n",
      "Epoch 2/15\n",
      "54000/54000 [==============================] - 39s 729us/sample - loss: 0.1086 - acc: 0.9672 - val_loss: 0.0553 - val_acc: 0.9862\n",
      "Epoch 3/15\n",
      "54000/54000 [==============================] - 42s 777us/sample - loss: 0.0831 - acc: 0.9742 - val_loss: 0.0489 - val_acc: 0.9865\n",
      "Epoch 4/15\n",
      "54000/54000 [==============================] - 46s 859us/sample - loss: 0.0677 - acc: 0.9792 - val_loss: 0.0405 - val_acc: 0.9887\n",
      "Epoch 5/15\n",
      "54000/54000 [==============================] - 34s 630us/sample - loss: 0.0611 - acc: 0.9805 - val_loss: 0.0362 - val_acc: 0.9912\n",
      "Epoch 6/15\n",
      "54000/54000 [==============================] - 35s 647us/sample - loss: 0.0525 - acc: 0.9842 - val_loss: 0.0358 - val_acc: 0.9903\n",
      "Epoch 7/15\n",
      "54000/54000 [==============================] - 34s 624us/sample - loss: 0.0494 - acc: 0.9845 - val_loss: 0.0370 - val_acc: 0.9893\n",
      "Epoch 8/15\n",
      "54000/54000 [==============================] - 43s 792us/sample - loss: 0.0459 - acc: 0.9851 - val_loss: 0.0319 - val_acc: 0.9913\n",
      "Epoch 9/15\n",
      "54000/54000 [==============================] - 34s 634us/sample - loss: 0.0431 - acc: 0.9867 - val_loss: 0.0300 - val_acc: 0.9922\n",
      "Epoch 10/15\n",
      "54000/54000 [==============================] - 42s 783us/sample - loss: 0.0403 - acc: 0.9871 - val_loss: 0.0306 - val_acc: 0.9922\n",
      "Epoch 11/15\n",
      "54000/54000 [==============================] - 33s 607us/sample - loss: 0.0398 - acc: 0.9870 - val_loss: 0.0321 - val_acc: 0.9915\n",
      "Epoch 12/15\n",
      "54000/54000 [==============================] - 33s 619us/sample - loss: 0.0367 - acc: 0.9878 - val_loss: 0.0271 - val_acc: 0.9937\n",
      "Epoch 13/15\n",
      "54000/54000 [==============================] - 34s 636us/sample - loss: 0.0349 - acc: 0.9886 - val_loss: 0.0270 - val_acc: 0.9925\n",
      "Epoch 14/15\n",
      "54000/54000 [==============================] - 49s 900us/sample - loss: 0.0332 - acc: 0.9896 - val_loss: 0.0274 - val_acc: 0.9927\n",
      "Epoch 15/15\n",
      "54000/54000 [==============================] - 38s 709us/sample - loss: 0.0315 - acc: 0.9894 - val_loss: 0.0266 - val_acc: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee9452b990>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.023136143504828215\n",
      "Test accuracy: 0.9927\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the trained model if needed:\n",
    "> model.save('mnist_2nn_keras')\n",
    "\n",
    "#### create a new model later with the saved model:\n",
    "> new_model = tf.keras.models.load_model('mnist_2nn_keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Both models built with PyTorch and TensorFlow-Keras, have around 98 to 99 percent accuracy for the MNIST dataset. Tuning the hyper-parameters would yield even better results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
