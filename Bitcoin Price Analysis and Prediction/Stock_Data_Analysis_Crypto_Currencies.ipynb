{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook by [Volkan Sonmez](http://www.pythonicfool.com/)  \n",
    "### Time series analysis and price prediction of Bitcoin made with ARIMA and LSTM \n",
    "##### [Pythonicfool GitHub Repository](https://github.com/volkansonmez/Exploratory_Data_Analysis_and_ML_Projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [Importing Data](#Importing_Data)\n",
    "    \n",
    "4. [ARIMA (Auto Integrated Moving Average Model)](#ARIMA)\n",
    "   \n",
    "5. [LSTM Model (Long Short Term Memory Model)](#LSTM_Model)\n",
    "\n",
    "6. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "    \n",
    "\n",
    "There are 2 machine learning algorithms used in this notebook to predict Bitcoin future prices. These algorithms are known as ARIMA and LSTM. \n",
    "\n",
    "> What is ARIMA?\n",
    "\n",
    "An ARIMA model is a class of statistical models for analyzing and forecasting time series data. ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average.\n",
    "\n",
    "> What is LSTM?\n",
    "\n",
    "The long short-term memory (LSTM) model is an advanced recurrent neural network (RNN) architecture used in the field of deep learning. It has feedback connections. It can process entire sequences of data. The LSTM machine learning model learns a function that maps a sequence of past observations as input to an output observation. \n",
    "\n",
    "\n",
    "As a generalization for stock data prediction, ARIMA yields better results in forecasting the short-term, whereas LSTM yields better results for long-term forecasting.\n",
    "\n",
    "#### Packages needed for building the models:\n",
    "\n",
    "This notebook uses several Python packages that come standard with the Anaconda Python distribution. \n",
    "The primary libraries you need to run this notebook are:\n",
    "\n",
    "* **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "* **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn**: The essential Machine Learning package in Python.\n",
    "* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **yfinance**: Yahoo financial data library. \n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "    conda install numpy pandas matplotlib sklearn yfinance tensorflow pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing_Data \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "* You will need: yfinance and datetime modules to import and view the stock data. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download these libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Download these additional libraries to access the trading data\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "start = datetime(end.year - 3, end.month, end.day) # last 3 years data\n",
    "print(start)\n",
    "# For a cleaner notebook, add these lines below\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the imported data into a variable that holds the dataframe and preview it in a simple chart\n",
    "df_btc = yf.download('BTC-USD', start = start, end = end)\n",
    "  \n",
    "# display\n",
    "plt.figure(figsize = (5,3))\n",
    "plt.title('Opening Prices from {} to {}'.format(start, end))\n",
    "plt.plot(df_btc['Open'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the properties of the dataframe\n",
    "\n",
    "print(df_btc.info())\n",
    "df_btc.plot(x= None, y='Adj Close', legend=True, figsize=(20,5)) \n",
    "df_btc[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS:\n",
    "# Repeat the same procedure for the Ethereum's price chart to visualize both Bitcoin and Ethereum prices together\n",
    "\n",
    "ETH = yf.download('ETH-USD', start = start, end = end)\n",
    "df_eth = pd.DataFrame(ETH)\n",
    "df_eth.plot(x= None, y= 'Close',legend=True, figsize=(20,5), color = 'red') # view the data\n",
    "print(df_eth.info())\n",
    "df_eth.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two plots on the same frame to visualize both Bitcoin and Ethereum prices together.\n",
    "\n",
    "# Compare these two stock closing values based on their 5 year performance\n",
    "x = df_eth.index[:] \n",
    "y1 = df_btc['Adj Close'][:]\n",
    "y2 = df_eth['Adj Close'][:]\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x,y1,label=\"Bitcoin\", )\n",
    "plt.plot(x,y2,label=\"Ethereum\")\n",
    "plt.title(\"Values Comparision of Bitcoin and Ethereum, 2019-2020\")\n",
    "plt.legend(title=\"SYMBOLS\")\n",
    "print('starting value for BTC-USD', y1[0], 'starting value for ETH-USD', y2[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart does not show a well-adjusted value comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the starting point of the crypto values to 1 dollar to compare their growth rate. \n",
    "\n",
    "readjusted_y1 = 1/y1[0] * y1\n",
    "readjusted_y2 = 1/y2[0] * y2\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x,readjusted_y1,label=\"Bitcoin\", )\n",
    "plt.plot(x,readjusted_y2,label=\"Ethereum\")\n",
    "plt.title(\"3 Years Growth Comparision for Bitcoin vs Ethereum\")\n",
    "plt.legend(title=\"SYMBOLS\")\n",
    "# print('Assuming initial investment for BTC-USD is', readjusted_y1[0], 'and for ETH-USD is', readjusted_y2[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above shows both cryptos' values over the last 3 years with their starting price as 1 dollar (as if your initial investment was 1 dollar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the trading volumes  \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.suptitle('Volume')\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "ax1.plot(df_btc.index, df_btc['Volume'], 'o-', color = 'blue')\n",
    "ax1.set_ylabel('Bitcoin')\n",
    "\n",
    "ax2.plot(df_eth.index, df_eth['Volume'], 'o-', color = 'red')\n",
    "ax2.set_ylabel('Ethereum')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trading volume of Ethereum is larger than Bitcoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "To be able to do a prediction on stocks or crypto data with the ARIMA model, we need to manipulate the data until we get some kind of a stationary pattern. A weekly, monthly, quarterly moving average of the data will usually show a stationary pattern. \n",
    "\n",
    "Most raw financial data of stock values or crypto prices have high p-values (high p-value means very highly volatile non-stationary data with an unpredictable pattern). If the p-value of the data is observed greater than 0.05, the time series is non-stationary. If the p-value of the data is high, create a new dataset that holds a moving average of a specific time frame. This will allow a prediction on a stationary pattern. \n",
    "\n",
    "The Augmented Dickey-Fuller test (ADF test) is a common statistical test used to test whether a given time series is stationary or not. Use this test to check the p-values of the raw data and the moving average data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an ARIMA model for prediction of Bitcoin's future prices\n",
    "\n",
    "# Check if the data is already stationary\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# ADF Test\n",
    "result = adfuller(y1, autolag='AIC')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'n_lags: {result[1]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "for key, value in result[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is highly non-stationary, we need to make it stationary by extracting patterns to feed it into a model. \n",
    "\n",
    "Most people try quarterly, weekly shifts as well. There is no great rule of thumb shift value for ANY STOCK/CRYPTO PRICE!\n",
    "\n",
    "To predict future values with ARIMA model, the p-value of the data needs to be below 0.05. \n",
    "\n",
    "This data is NOT showing an exponential pattern. So, the log transformation of the data would punish the good results too much. It is also certainly NOT seasonal data. Therefore, seasonal transformation is NOT an option. Out of the 3 well-known options which are: Differencing, Seasonal Differencing, and Transformation, \"differencing\" is the best option for working on this dataset. Shifting the original data into a monthly pattern would be a reasonable approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the data's p value to below 0.05 by taking the 30 day average so that stationary values can be obtained. \n",
    "\n",
    "df_btc_new = y1.copy()\n",
    "# Smooth the data with the mean of 30 instances with a sliding window\n",
    "shifted_df_btc_new = df_btc_new.shift(30)\n",
    "df_btc_new = df_btc_new - shifted_df_btc_new\n",
    "df_btc_new.dropna(inplace=True) # get rid of the none values\n",
    "\n",
    "ma_new = df_btc_new.rolling(window=30).mean() # new data moving averege mean\n",
    "ma_new.dropna(inplace=True)\n",
    "std_new = df_btc_new.rolling(window=30).std() # new data std \n",
    "std_new.dropna(inplace=True)\n",
    "\n",
    "# view the chart \n",
    "plt.title(\"Shifted Bitcoin Closing Values vs MA vs Std\")\n",
    "plt.plot(df_btc_new, color = 'blue', )\n",
    "plt.plot(ma_new, color = 'grey')\n",
    "plt.plot(std_new, color = 'red')\n",
    "\n",
    "# After shifting the data 30 days, we have a pattern that would work better for ARIMA model. \n",
    "# df_btc_new.plot(x = None, y = 'y', figsize=(20,5), color = 'blue' )\n",
    "\n",
    "# ADF Test Again\n",
    "result_log = adfuller(df_btc_new, autolag='AIC')\n",
    "print(f'ADF Statistic: {result_log[0]}')\n",
    "print(f'n_lags: {result_log[1]}')\n",
    "print(f'p-value: {result_log[1]}')\n",
    "for key, value in result[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')   \n",
    "\n",
    "# p value is now below 0.05 so we can feed this new shifted data to the ARIMA model.\n",
    "df_btc_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Regressive Integrated Moving Average Model is made of: (AR(Auto Regressive) + MA(Moving Average))\n",
    "\n",
    "ACF (Auto Correlation Function) & PACF (Partial Auto Correlation Function) graphs are used to find Q & P values. P value is showing auto regressive lags and Q value is showing moving average.  \n",
    "\n",
    "This is how to get the Q, D & P parameters of ARIMA traditionally: \n",
    "* From PACF (at y=0) we get P value. \n",
    "* From ACF (at y=0) we get Q value. \n",
    "* D value is the order of differentiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ARIMA: \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Try grid search for best P, D, Q parameters since this time series is too hard for getting the right params.\n",
    "p_values = range(1, 12)\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "\n",
    "# Split the data into the train and test sets\n",
    "# intentionally allocating a large portion to the training data since values seem too volitile the last 3 months. \n",
    "index = int(len(df_btc_new) * 0.90) \n",
    "train_data = df_btc_new[:index]\n",
    "test_data = df_btc_new[index:]\n",
    "\n",
    "# Write a simple function to calculate the mean square error between the testing set and trained model forecast\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def run_ARIMA(train, test, p, d, q):\n",
    "    order = (p,d,q)\n",
    "    model = ARIMA(train, order) # this is how the model is fit \n",
    "    model_fit = model.fit(disp=0)\n",
    "    fc, se, conf = model_fit.forecast(len(test))\n",
    "    fc = np.array(fc)\n",
    "    test = np.array(test)\n",
    "    total_error = mean_squared_error(test, fc)\n",
    "    return total_error\n",
    " \n",
    "    \n",
    "# Do a grid search on the best parameters to find the minimum error between the training and testing datasets\n",
    "min_error = np.inf \n",
    "best_params = None\n",
    "for p in p_values:\n",
    "    for d in d_values:\n",
    "        for q in q_values:\n",
    "            try:\n",
    "                error = run_ARIMA(train_data, test_data, p, d, q)\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    best_p = p\n",
    "                    best_d = d\n",
    "                    best_q = q\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print('Best ARIMA parameters are: ' , best_p, best_d, best_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model with the best parameters and fit them into the model.\n",
    "\n",
    "model = ARIMA(df_btc_new, order = (best_p, best_d, best_q))  \n",
    "fitted = model.fit(disp = -1)  \n",
    "\n",
    "# Make a forecast based on the tweaked (shifted) data\n",
    "fc, se, conf = fitted.forecast(30, alpha=0.05)  # 95% confidence level\n",
    "\n",
    "print(len(fc), fc[:5], se[:5], conf[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the forecast to pandas series and view the last year's actual data and the next 30 days projection\n",
    "\n",
    "fc_series = np.array(fc) \n",
    "data_last_year = np.array(df_btc['Adj Close'][-365:])\n",
    "fc_series += data_last_year[-1] # start from the last day\n",
    "data_with_fc = []\n",
    "for i in range(len(data_last_year)):\n",
    "    data_with_fc.append(data_last_year[i])\n",
    "for i in range(len(fc_series)):\n",
    "    data_with_fc.append(fc_series[i])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.suptitle('BITCOIN Price')\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(20)\n",
    "\n",
    "ax1.plot(data_last_year, 'o-', color = 'blue')\n",
    "ax1.set_ylabel('Last 365 Days')\n",
    "\n",
    "ax2.plot(data_with_fc, 'o-', color = 'red')\n",
    "ax2.set_ylabel('Next 30 Days Added')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows a prediction of an ARIMA model for the next 30 days based on a 30 day moving average integrated model for bitcoin prices. It is showing a large error (about a 5% shift with a vertical line) in the chart since the predicted values do not start from the last day's value in the dataset. It is also a very costly procedure. We will skip following the same procedure for predicting the ETH prices. Let's move on to another model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM_Model\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Create a Keras LSTM Model and train it with BITCOIN price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_btc['Adj Close'].values\n",
    "print(dataset[:-10])\n",
    "# Convert the dataframe to a numpy array\n",
    "dataset = np.array(dataset)\n",
    "dataset = dataset[:,np.newaxis]\n",
    "# Get the number of rows to train the model on\n",
    "split_index = int(len(dataset) * 0.8)\n",
    "print(split_index)\n",
    "print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "print(scaled_data[:5], scaled_data[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own target, based on the time series. \n",
    "# In this notebook, the targets are chosen for the previous 30 days values.\n",
    "\n",
    "def create_LSTM_dataset(dataset):\n",
    "    x_dataset = []\n",
    "    y_dataset = []\n",
    "    # start from the 30th day since we are more interested in the later data \n",
    "    for i in range(30, len(dataset)):\n",
    "        x_dataset.append(dataset[i-30:i])\n",
    "        y_dataset.append(dataset[i])\n",
    "    # for the first 30 days, only insert the average of the first 30 days as they are. \n",
    "    # LSTM will forget them eventually so no need for taking the \n",
    "    # average or weighted averages of smaller time periods\n",
    "    # first_30_days = dataset[:30]\n",
    "    return np.array(x_dataset), np.array(y_dataset)\n",
    "\n",
    "x, y = create_LSTM_dataset(scaled_data)\n",
    "# print(x[0], y[0])\n",
    "print(x.shape, y.shape)  # LSTM requires a dataset with 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the split for training and test sets\n",
    "\n",
    "split_index = int(len(scaled_data) * 0.85)\n",
    "trainX, testX = x[:split_index], x[split_index:]\n",
    "trainY, testY = y[:split_index] , y[split_index:]\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "# print(testX[0], testY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with Keras \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "#Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape= (trainX.shape[1], trainX.shape[2])))\n",
    "model.add(LSTM(100, return_sequences= False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "model.fit(x = trainX, y = trainY, validation_data = (testX, testY), epochs = 5, batch_size = 30, verbose = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the loss is quite small, there is no need to increase epoch size and overfit the data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a prediction with the test set\n",
    "\n",
    "test_predictions = model.predict(testX)\n",
    "print(test_predictions[:5])\n",
    "predictions = scaler.inverse_transform(test_predictions)\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both the actual data (gray line) and predictions on the test set (blue line)\n",
    "\n",
    "concat = []\n",
    "# shift the starting point all the way to the end of the chart\n",
    "for i in range(len(trainX) + 29):\n",
    "    concat.append(None)\n",
    "for i in range(len(predictions)):\n",
    "    concat.append(predictions[i])\n",
    "\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(concat, color = 'blue')\n",
    "plt.plot(dataset, color = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions follow the actual pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not going to be very realistic but let's make a prediction for the next 7 days.\n",
    "\n",
    "print(x.shape)\n",
    "x_last_month = x[-7:, :, :]  #  arr[np.newaxis, :] \n",
    "print(x_last_month.shape)\n",
    "next_month_prediction = model.predict(x_last_month)\n",
    "prediction = scaler.inverse_transform(next_month_prediction)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(prediction, color = 'red')\n",
    "\n",
    "print(prediction[-1]) # where it is headed to the next 30 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Create the same sequential model with pytorch if you like instead of Keras. It will be the same outcome. \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # Reminder: np.array does not work with pytorch, data structure has to be a tensor before they are fed into the model.\n",
    "    trainY = torch.Tensor(trainY) \n",
    "    trainX = torch.Tensor(trainX)\n",
    "    print(trainY.shape, trainX.shape)\n",
    "\n",
    "    class LSTM(nn.Module):\n",
    "        def __init__(self, input_size=30, hidden_layer_size=50, output_size=1):\n",
    "            super().__init__()\n",
    "            self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "            self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "            self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "            self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                                torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "        def forward(self, input_seq):\n",
    "            lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "            predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "            return predictions[-1]\n",
    "\n",
    "    model = LSTM()\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "This notebook was made to teach two basic prediction modeling using bitcoin prices for beginner machine learning students. The ARIMA (statistical) model was built on monthly moving average data since the raw data was not stationary. The LSTM (deep learning) model was easy, fast, and cheap to build. Both of them have their error-prone building blocks and need fine-tuning with more complex architectures to making predictions with higher accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
